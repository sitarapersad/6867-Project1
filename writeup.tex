\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\usepackage{color}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage[export]{adjustbox}
\usepackage{mathtools}
\usepackage{ textcomp }
\usepackage{enumerate}
\usepackage[T1]{fontenc}
\usepackage{subfigure}
\usepackage{program}

\usepackage{cite}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

%%%% Custom sectioning
\usepackage{sectsty}
\allsectionsfont{\raggedright \normalfont\scshape}
\subsectionfont{\raggedright \normalfont \scshape}

\usepackage[parfill]{parskip}
\newtheorem{definition}{Definition}[section]
\newcommand{\ex}[1]{\mathbb{E}[{ #1}]} %expectation variable
\newcommand{\eval}[2]{\bigg\vert_{#1}^{#2}}
\newcommand{\?}[0]{\vert}
\newcommand{\dt}[0]{\delta}



\begin{document}

\title{Numerical Methods For Estimation\\
	6.867 HW 1}

\author{Anonymous}


\date{September 28, 2016}

\maketitle


%-------------------------------------------------%
\section{Gradient Descent}
%-------------------------------------------------%

\subsection{Basic Gradient Descent}

In this section, we implement a basic generic gradient descent algorithm. The goal of gradient descent is to optimize some differential cost function, by repeatedly adjusting our 'guess' for the optimizing value. Intuitively, this is achieved by iteratively 'moving' our guess in the direction of the greatest rate of decrease of the error function, scaled by a learning rate, towards some minimum value.


We implemented basic batch gradient descent with a random initialization, and adjusted to parameters of learning rate and convergence threshold to see how the performance of the algorithm varied when tested on the negative gaussian function, and the quadratic bowl.


%-------------------------------------------------%
\section{Linear Basis Function Regression}
%-------------------------------------------------%


%-------------------------------------------------%
\section{Ridge Regression}
%-------------------------------------------------%

In this section, we implement ridge regression (Bishop equation 3.27, 3.28). Ridge regression aims to mitigate the effect of overfitting by adding a regularization term that causes weights in \textbf{w} to shrink to zero, unless there is substantial evidence in the data against this.








\end{document}

