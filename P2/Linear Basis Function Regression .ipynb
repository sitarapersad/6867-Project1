{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "import numpy as np\n",
    "import pandas \n",
    "import linearBasis as lb\n",
    "import gradientDescent as gd\n",
    "import loadFittingDataP2 as fitData\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Let’s consider the linear combination of basis function class of regression models.  \n",
    "We know how to get closed-form solutions for the maximum likelihood weight vector (see Bishop equation 3.15,\n",
    "Murphy section 7.2 and equation 7.16). We’ll use this problem to “benchmark” the gradient descent\n",
    "solution.\n",
    "\n",
    "\n",
    "We have provided you with a text file (curvefittingp2.txt) that has 11 data points for you to\n",
    "fit.  The function we used for generating these data is $ y(x) = \\cos(\\pi x) + 1.5 \\cos(2\\pi x)$, with a little\n",
    "added noise.  We have also given you code to read this data and some code illustrating how to\n",
    "generate plots, both in Python and Matlab.\n",
    "\n",
    "1.  Assume that we do not know the real basis for these points and want to try out a polynomial\n",
    "fitting.  Write a procedure for computing the maximum likelihood weight vector given (a) an\n",
    "array of 1-dimensional data points X, (b) a vector of Y values,  and (c) the value of M, the\n",
    "maximum order of a simple polynomial basis $\\phi_{0}(x) = 1 ,  \\phi_{1}(x) = x,  ..., \\phi_{M}(x) = x^{M}$ .\n",
    "Test your solution by replicating the plots above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-d8d513aa899d>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-d8d513aa899d>\"\u001b[1;36m, line \u001b[1;32m11\u001b[0m\n\u001b[1;33m    TO DO:\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Implement the solution and test against given data for various polynomial bases. \n",
    "\n",
    "#grab test data\n",
    "X, Y= fitData.getData()\n",
    "#compute MLE weight vector\n",
    "\n",
    "for M in [0,1,2,10]:\n",
    "    w = computePolynomialWeight(X, Y, M)\n",
    "    # plot data to visualize fit\n",
    "    \n",
    "    TO DO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, write functions to compute the sum of squares error (SSE) (and its derivative) for a data\n",
    "set and a hypothesis, specified by the list of M polynomial basis functions and a weight vector.\n",
    "Verify your gradient using the numerical derivative code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For various values of delta, calculate the gradient using the closed form and the approximation of SSE to verify against each other\n",
    "\n",
    "X, Y= lb.getData()\n",
    "\n",
    "w = \n",
    "M_list = \n",
    "\n",
    "diffs = []\n",
    "deltas = [1e-4]\n",
    "\n",
    "for d in deltas:\n",
    "    true = lb.computeSSE(X, Y, M_list, w)\n",
    "    approx_fn = lambda w: lb.computeSSE(X, Y, M_list, w)\n",
    "    approx = gd.approximateGradient(w, approx_fn, delta)\n",
    "    \n",
    "    diffs.append(true-approx)\n",
    "\n",
    "#Plot the difference in gradient value vs delta value\n",
    "pl.plot(x=deltas, y=diffs)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use batch gradient descent on the SSE function on some values of M. Describe your experience\n",
    "with initial guesses, step sizes and convergence thresholds. Compare with using SGD on the\n",
    "same data. Explain your results in terms of the properties of the function being optimized and\n",
    "the properties of the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-e8bf82a3c8b8>, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-e8bf82a3c8b8>\"\u001b[1;36m, line \u001b[1;32m17\u001b[0m\n\u001b[1;33m    print 'Batch gradient descent converged to ', best_value, ' in ', len(norm), ' iterations'.\u001b[0m\n\u001b[1;37m                                                                                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Run batch gradient descent on the SSE function for various values of M\n",
    "# Play around with different options for initial_guesses, step_sizes and convergence.\n",
    "\n",
    "X, Y= lb.getData()\n",
    "M_list = []\n",
    "initial_guess\n",
    "\n",
    "objective_fn = lambda w: lb.computeSSE(X, Y, M_list, w)\n",
    "gradient_fn = lambda w: lb.computeSSE_prime(X, Y, M_list, w)\n",
    "\n",
    "best_guess, best_value, guess, fxn, norm = gd.gradientDescent(objective_fn,\n",
    "                                                              gradient_fn,\n",
    "                                                              initial_guess, \n",
    "                                                              step_size=1000,\n",
    "                                                              convergence = 1e-10)\n",
    "\n",
    "print 'Batch gradient descent converged to ', best_value, ' in ', len(norm), ' iterations'\n",
    "\n",
    "# Run stochastic gradient descent on the given dataset then compare it with the previous batch gradient descent\n",
    "best_guess, best_value, guess, fxn, norm = stochasticGradientDescent(x = X,\n",
    "                                                                     y = Y, \n",
    "                                                                     objective_fn = objective_fn,\n",
    "                                                                     gradient_fn = gradient_fn,\n",
    "                                                                     initial_guess= initial_guess, \n",
    "                                                                     step_size=1000,\n",
    "                                                                     convergence = 1e-4)\n",
    "\n",
    "print 'Stochastic gradient descent converged to ', best_value, ' in ', len(norm), ' iterations'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that you know the basis functions were cosine functions rather than polynomials,\n",
    "such that φ1(x) = cos(πx), φ2(x) = cos(2πx), . . . , φM(x) = cos(Mπx), but you do not know\n",
    "which cosines you should use. Perform the maximum likelihood weight vector calculation\n",
    "for the cosine basis functions for the first eight φM(x). How does your weight vector compare\n",
    "to the true values we used to generate the data? (If you have prior beliefs on certain\n",
    "property (e.g., sparsity) of your regression coefficients, you can enforce it with regularization\n",
    "techniques, which we will explore in Q4. You don’t need to implement it for this question.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate the MLE weight vector from the given data and compare it to the true value\n",
    "\n",
    "for M in range(9):\n",
    "    w = lb.computeCosWeight(X, Y, M = 8)\n",
    "    print 'M: ',M\n",
    "    print 'w:', w \n",
    "    print"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
