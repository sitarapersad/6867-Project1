{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "import numpy as np\n",
    "import pandas \n",
    "import gradientDescent as gd\n",
    "import loadParametersP1 as params\n",
    "import loadFittingDataP1 as fitData\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Implement a basic gradient descent procedure to minimize scalar functions of a vector argument.\n",
    "Write it generically, so that you can easily specify the objective function and the function\n",
    "to compute the gradient. You should be able to specify the initial guess, the step size (or learning\n",
    "rate) and the convergence criterion (e.g., a threshold such that the algorithm terminates\n",
    "when the difference in objective function value on two successive steps is below this threshold\n",
    "or when the norm of the gradient is below this threshold). For this question, you can use a\n",
    "fixed step size in your implementation. Test your gradient descent procedure on two functions\n",
    "with the parameters provided by us in parametersp1.txt (for data files, we also provide basic\n",
    "reading scripts for MATLAB and Python): the negative of a Gaussian function and a quadratic\n",
    "bowl. Discuss (and illustrate) the effect of the choice of starting guess, the step size, and the\n",
    "convergence criterion on the resulting solution, as well as how the norm of the gradient evolves\n",
    "through the iteration.\n",
    "\n",
    "Things to Do:\n",
    "\n",
    "1. Test gradient descent procedure on the given functions and parameteres\n",
    "\n",
    "2. Study the effect of the choice of\n",
    "    1. starting guess\n",
    "    2. step size\n",
    "    3. convergence criteria\n",
    "3. Study how the norm of the gradient evolves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load parameters\n",
    "gaussMean,gaussCov,quadBowlA,quadBowlb = params.getData()\n",
    "\n",
    "# Test implementation of gradient descent on Gaussian function\n",
    "\n",
    "objective_fn = lambda x : gd.computeGaussian(x, gaussMean, gaussCov)\n",
    "gradient_fn = lambda x: gd.differentiateGaussian(x, gaussMean, gaussCov)\n",
    "                                                                                        \n",
    "best_guess, best_value, guess, fxn, norm = gd.gradientDescent(objective_fn,\n",
    "                                                              gradient_fn,\n",
    "                                                              initial_guess= [np.random.randint(0,100),np.random.randint(0,100)], \n",
    "                                                              step_size=1000,\n",
    "                                                              convergence = 1e-4)\n",
    "print 'Gaussian converged to: ', best_value, best_guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot how the norm changes over time \n",
    "print 'Plotting norm'\n",
    "pl.plot(x=range(len(norm)), y= norm)\n",
    "pl.show()\n",
    "\n",
    "# Plot how the function value changes over time\n",
    "pl.plot(x=range(len(fxn)), y= fxn)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, test the implementation of the quadratic bowl using the given parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test implementation of gradient descent on the Quadratic Bowl\n",
    "\n",
    "objective_fn = lambda x : gd.computeQuadBowl(x,quadBowlA,quadBowlb)\n",
    "gradient_fn = lambda x: gd.differentiateQuadBowl(x, quadBowlA,quadBowlb)\n",
    "\n",
    "best_guess, best_value, guess, fxn, norm = gd.gradientDescent(objective_fn,\n",
    "                                                              gradient_fn,\n",
    "                                                              initial_guess= [np.random.randint(-100,100),np.random.randint(-100,100)], \n",
    "                                                              step_size=1000,\n",
    "                                                              convergence = 1e-10)\n",
    "print 'Quadratic Bowl converged to: ', best_value, best_guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot how the norm changes over time \n",
    "print 'Plotting norm'\n",
    "pl.plot(x=range(len(norm)), y= norm)\n",
    "pl.show()\n",
    "\n",
    "# Plot how the function value changes over time\n",
    "pl.plot(x=range(len(fxn)), y= fxn)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of starting guess, step size and convergence criteria\n",
    "Discuss (and illustrate) the effect of the choice of starting guess, the step size, and the\n",
    "convergence criterion on the resulting solution, as well as how the norm of the gradient evolves\n",
    "through the iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test the effect of the step size on the number of iterations to convergence and on the solution converged to\n",
    "\n",
    "def test_step(step_size):\n",
    "    best_guess, best_value, guess, fxn, norm = gd.gradientDescent(objective_fn,\n",
    "                                                              gradient_fn,\n",
    "                                                              initial_guess= [np.random.randint(-100,100),np.random.randint(-100,100)], \n",
    "                                                              step_size=step_size,\n",
    "                                                              convergence = 1e-10)\n",
    "    return best_value, len(fxn) #function value, number iterations\n",
    "\n",
    "step_sizes = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "\n",
    "values= []\n",
    "iters = []\n",
    "\n",
    "for step_size in step_sizes:\n",
    "    val, its = test_step(step_size)\n",
    "    values.append(val)\n",
    "    iters.append(its)\n",
    "    \n",
    "# Plot the variation in convergence value as a function of step_size\n",
    "\n",
    "# Plot the variation in number of iterations as a function of step_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test the effect of the convergence criteria on the number of iterations to convergence and on the solution converged to\n",
    "\n",
    "def test_convergence(rate):\n",
    "    best_guess, best_value, guess, fxn, norm = gd.gradientDescent(objective_fn,\n",
    "                                                              gradient_fn,\n",
    "                                                              initial_guess= [np.random.randint(-100,100),np.random.randint(-100,100)], \n",
    "                                                              step_size= 1000,\n",
    "                                                              convergence = rate)\n",
    "    return best_value, len(fxn) #function value, number iterations\n",
    "\n",
    "criteria = [1e-10, 1e-8, 1e-6, 1e-4, 1e-2, 1, 10]\n",
    "\n",
    "values= []\n",
    "iters = []\n",
    "\n",
    "for rate in criteria:\n",
    "    val, its = test_convergence(rate)\n",
    "    values.append(val)\n",
    "    iters.append(its)\n",
    "    \n",
    "# Plot the variation in convergence value as a function of step_size\n",
    "\n",
    "# Plot the variation in number of iterations as a function of step_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test the effect of the starting guess on the number of iterations to convergence and on the solution converged to\n",
    "\n",
    "TO DO\n",
    "\n",
    "def test_convergence(rate):\n",
    "    best_guess, best_value, guess, fxn, norm = gd.gradientDescent(objective_fn,\n",
    "                                                              gradient_fn,\n",
    "                                                              initial_guess= [np.random.randint(-100,100),np.random.randint(-100,100)], \n",
    "                                                              step_size= 1000,\n",
    "                                                              convergence = rate)\n",
    "    return best_value, len(fxn) #function value, number iterations\n",
    "\n",
    "criteria = [1e-10, 1e-8, 1e-6, 1e-4, 1e-2, 1, 10]\n",
    "\n",
    "values= []\n",
    "iters = []\n",
    "\n",
    "for rate in criteria:\n",
    "    val, its = test_convergence(rate)\n",
    "    values.append(val)\n",
    "    iters.append(its)\n",
    "    \n",
    "# Plot the variation in convergence value as a function of step_size\n",
    "\n",
    "# Plot the variation in number of iterations as a function of step_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The gradient function may not always look as simple and clean as the ones provided above. \n",
    "A common way to check if one’s gradient evaluation is correct or not is to use the central difference\n",
    "approximation (see the “Finite difference” article in Wikipedia) to numerically evaluate\n",
    "the gradient at various points. Write code to approximate the gradient of a function numerically\n",
    "at a given point using this method. Verify the gradient values on the functions you used\n",
    "in the question above by comparing the closed-form and numerical gradients at various points.\n",
    "Discuss the effect of changing the difference step (or $\\delta$) on the accuracy of the gradient\n",
    "evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For various values of delta, calculate the gradient using the closed form and the approximation of Gaussian\n",
    "# Use 1-D points for simpler visualization\n",
    "\n",
    "diffs = []\n",
    "deltas = [1e-8, 1e-6, 1e-4, 1e-2,1,10,1e2,1e4]\n",
    "\n",
    "for d in deltas:\n",
    "    point = -100 + 200* np.random.random()\n",
    "\n",
    "    true = differentiateGaussian(point, gaussMean, gaussCov)\n",
    "    approx_fn = lambda x: Gaussian(x, gaussMean, gaussCov)\n",
    "    approx = approximateGradient(point, approx_fn, d)\n",
    "    \n",
    "    diffs.append(true-approx)\n",
    "    \n",
    "#Plot the difference in gradient value vs delta value\n",
    "pl.plot(x=deltas, y=diffs)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For various values of delta, calculate the gradient using the closed form and the approximation of Quadratic Bowl\n",
    "# Use 1-D points for simpler visualization\n",
    "\n",
    "diffs = []\n",
    "deltas = [1e-8, 1e-6, 1e-4, 1e-2,1,10,1e2,1e4]\n",
    "\n",
    "for d in deltas:\n",
    "    point = -100 + 200* np.random.random()\n",
    "\n",
    "    true =differentiateQuadBowl(point,quadBowlA,quadBowlb)\n",
    "    approx_fn = lambda x: computeQuadBowl(x,quadBowlA,quadBowlb)\n",
    "    approx = approximateGradient(point, approx_fn, delta)\n",
    "    \n",
    "    diffs.append(true-approx)\n",
    "\n",
    "#Plot the difference in gradient value vs delta value\n",
    "pl.plot(x=deltas, y=diffs)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. In machine learning and statistical estimation context, batch gradient descent uses samples from the full training set for each parameter update. \n",
    "Thus, each iteration would be slow for a large dataset. In addition, the native form of batch gradient descent does not provide a way to incorporate new data efficiently (i.e., in an online setting). Stochastic gradient descent (SGD)\n",
    "addresses these problems by using stochastic approximation on the gradient term (See Bishop\n",
    "3.1.3, 5.2.4 and Wikipedia). In this question, we will use both batch gradient descent and SGD\n",
    "on a least square fitting problem.\n",
    "The dataset is provided in fittingdatap1 x.txt and fittingdatap1 y.txt. Each row of X and\n",
    "y represents a single data sample pair (x(i), y(i)) and your goal is to find a coefficient vector θ\n",
    "that minimizes the least square error of J(θ) = ||Xθ − y||2. In this question, you don’t need to augment X\n",
    "with 1’s as we did in the lecture. Notice that J(θ) is natively a function over the whole dataset\n",
    "and there is a closed-form solution to this problem (Bishop Equation (3.15)), which would be\n",
    "useful for checking your answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Use batch gradient descent on J(θ) with a fixed step size. This would be the case where the\n",
    "gradient of the cost function for the entire training dataset is used in each parameter update.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run batch gradient descent with a squared error objective function\n",
    "\n",
    "#Load data set\n",
    "X, Y = fitData.getData()\n",
    "\n",
    "objective_fn = lambda x : gd.computeSquaredLoss(X, Y, theta)\n",
    "gradient_fn = lambda x: gd.differentiateSquaredLoss(X,Y,theta)\n",
    "                                                                                        \n",
    "best_guess, best_value, guess, fxn, norm = gd.gradientDescent(objective_fn,\n",
    "                                                              gradient_fn,\n",
    "                                                              initial_guess= [np.random.randint(0,100),np.random.randint(0,100)], \n",
    "                                                              step_size=1000,\n",
    "                                                              convergence = 1e-4)\n",
    "\n",
    "print 'Least squares converged to: ', best_guess, best_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) In contrast to batch gradient descent, which uses all training data in every parameter update,\n",
    "SGD performs a parameter update based on a single training example:\n",
    "θt+1 = θt − ηt∇θJ(θt\n",
    "; x\n",
    "(i)\n",
    ", y(i)\n",
    "),\n",
    "where ηt\n",
    "is the learning rate for each iteration t = 1, 2, ... and J(θt\n",
    "; x\n",
    "(i)\n",
    ", y(i)\n",
    ") = (x\n",
    "(i)T\n",
    "θt−y\n",
    "(i)\n",
    ")\n",
    "2\n",
    "is\n",
    "the objective function in terms of a single data sample. Write a general procedure that performs\n",
    "SGD on this dataset. You should first derive the point-wise (with respect to a single data\n",
    "sample) gradient function, ∇θJ(θt\n",
    "; x\n",
    "(i)\n",
    ", y(i)\n",
    "). Then write a procedure to perform parameter\n",
    "update by iterating through the samples in the dataset for some number of rounds until a\n",
    "stopping criterion is reached (such as convergence of the full objective function over all data\n",
    "points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Compare the behavior of the two implementations in terms of accuracy and number of evaluations\n",
    "of the point-wise gradient, ∇θJ(θt\n",
    "; x\n",
    "(i)\n",
    ", y(i)\n",
    "). That is, each iteration of batch gradient\n",
    "descent takes n (the number of total samples) evaluations of the point-wise gradients, whereas\n",
    "each iteration of SGD takes one evaluation of it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run stochastic gradient descent on the given dataset then compare it with the previous batch gradient descent\n",
    "best_guess, best_value, guess, fxn, norm = stochasticGradientDescent(x = X,\n",
    "                                                                     y = Y, \n",
    "                                                                     objective_fn = objective_fn,\n",
    "                                                                     gradient_fn = gradient_fn,\n",
    "                                                                     initial_guess= [np.random.randint(0,100),np.random.randint(0,100)], \n",
    "                                                                     step_size=1000,\n",
    "                                                                     convergence = 1e-4)\n",
    "\n",
    "print 'Stochastic gradient descent converged to ', best_value, ' in ', len(norm), ' iterations'.\n",
    "\n",
    "# Plot how the norm changes over time \n",
    "pl.plot(x=range(len(norm)), y= norm)\n",
    "pl.show()\n",
    "\n",
    "# Plot how the function value changes over time\n",
    "pl.plot(x=range(len(fxn)), y= fxn)\n",
    "pl.show()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
